{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Python_ package [neurolab](https://github.com/zueve/neurolab) provides a working environment for ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement a neural network having a single layer of perceptrons (apart from input units) using _neurolab_ package as an instance of the class `newp`. In order to do so we need to provide the following parameters:\n",
    "* `minmax`: a list with the same length as the number of input neurons. The $i$-th element on this list is a list of two numbers, indicating the range of input values for the $i$-th neuron.\n",
    "* `cn`: number of output neurons.\n",
    "* `transf`: activation function (default value is threshold).\n",
    "\n",
    "Therefore, when we choose 1 as the value of parameter `cn`, we will be representing a simple perceptron having as many inputs as the length of the list associated to `minmax`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by creating a simple perceptron with two inputs, both of them ranging in $[0, 1]$, and with threshold activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurolab import net\n",
    "\n",
    "perceptron = net.newp(minmax=[[0, 1], [0, 1]], cn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instance that we just created has the following attributes:\n",
    "* `inp_minmax`: range of input values.\n",
    "* `co`: number of output neurons.\n",
    "* `trainf`: training function (the only one specific for single-layer perceptrons is the Delta rule).\n",
    "* `errorf`: error function (default value is half of SSE, _sum of squared errors_)\n",
    "\n",
    "The layers of the neural network (input layer does not count, thus in our example there is only one) are stored in a list associated with the attribute `layers`. Each layer is an instance of the class `Layer` and has the following attributes:\n",
    "* `ci`: number of inputs.\n",
    "* `cn`: number of neurons on it.\n",
    "* `co`: number of outputs.\n",
    "* `np`: dictionary with an element `'b'` that stores an array with the neurons' biasses (terms $a_0 w_0$, default value is 0) and an element `'w'` that stores an array with the weights associated with the incoming connections arriving on each neuron (default value is 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [0. 1.]]\n",
      "1\n",
      "Trainer(TrainDelta)\n",
      "<neurolab.error.SSE object at 0x000002017E2B97C0>\n",
      "2\n",
      "1\n",
      "1\n",
      "{'w': array([[0., 0.]]), 'b': array([0.])}\n"
     ]
    }
   ],
   "source": [
    "print(perceptron.inp_minmax)\n",
    "print(perceptron.co)\n",
    "print(perceptron.trainf)\n",
    "print(perceptron.errorf)\n",
    "layer = perceptron.layers[0]\n",
    "print(layer.ci)\n",
    "print(layer.cn)\n",
    "print(layer.co)\n",
    "print(layer.np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us train the perceptron so that it models the logic gate _and_.\n",
    "\n",
    "First of all, let us define the training set. We shall do it indicating on one hand an array or list of lists with the imput values corresponding to the examples, and on the other hand a different array or list of lists with the expected ouput for each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "input_values = numpy.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "expected_outcomes = numpy.array([[0], [0], [0], [1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `step` allows us to calculate the output of the neural network for a single example, and the method `sim` for all the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron.step([1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron.sim(input_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check which is the initial error of the perceptron, before the training.\n",
    "\n",
    "__Important__: the arguments of the error function must be arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron.errorf(expected_outcomes, perceptron.sim(input_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us next proceed to train the perceptron. We shall check that, as expected (since the training set is linearly separable), we are able to decrease the value of the error down to zero.\n",
    "\n",
    "__Note__: the method `train` that runs the training algorithm on the neural network returns a list showing the value of the network error after each of the _epochs_. More precisely, an epoch represents the set of operations performed by the training algorithm until all the examples  of the training set have been considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The goal of learning is reached\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5, 1.5, 1.0, 0.5, 1.0, 0.0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron.train(input_values, expected_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w': array([[0.02, 0.01]]), 'b': array([-0.02])}\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(perceptron.layers[0].np)\n",
    "print(perceptron.errorf(expected_outcomes, perceptron.sim(input_values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed forward perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package _neurolab_ implements a feed forward artificial neural network as an instance of the class `newff`. In order to do so, we need to provide the following parameters:\n",
    "* `minmax`: a list with the same length as the number of input neurons. The $i$-th element on this list is a list of two numbers, indicating the range of input values for the $i$-th neuron.\n",
    "* `cn`: number of output neurons.\n",
    "* `transf`: activation function (default value is threshold).\n",
    "\n",
    "* `size`: a list with the same length as the number of layers (except the input layer). The $i$-th element on this list is a number, indicating the number of neurons for the $i$-th layer.\n",
    "* `transf`: a list with the same length as the number of layers (except the input layer). The $i$-th element on this list is the activation function (default value is [hyperbolic tangent](https://en.wikipedia.org/wiki/Hyperbolic_functions) for the neurons of the $i$-th layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us create a neural network with two inputs ranging over $[0, 1]$, one hidden layer having two neurons and an output layer with only one neuron. All neurons should have the sigmoid function as activation function (you may look for further available activation functions at https://pythonhosted.org/neurolab/lib.html#module-neurolab.trans)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurolab import trans\n",
    "\n",
    "sigmoid_act_fun = trans.LogSig()\n",
    "my_net = net.newff(minmax=[[0, 1], [0, 1]], size=[2, 1], transf=[sigmoid_act_fun]*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instance that we just created has the following attributes:\n",
    "* `inp_minmax`: range of input values.\n",
    "* `co`: number of output neurons.\n",
    "* `trainf`: training function (default value is [Broyden–Fletcher–Goldfarb–Shanno algorithm](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)).\n",
    "* `errorf`: error function (default value is half of SSE, _sum of squared errors_)\n",
    "\n",
    "The layers of the neural network (input layer excluded) are stored in a list associated with the attribute `layers`. Each layer is an instance of the class `Layer` and has the following attributes:\n",
    "* `ci`: number of inputs.\n",
    "* `cn`: number of neurons on it.\n",
    "* `co`: number of outputs.\n",
    "* `np`: dictionary with an element `'b'` that stores an array with the neurons' biasses (terms $a_0 w_0$) and an element `'w'` that stores an array with the weights associated with the incoming connections arriving on each neuron. Default values for the biasses and the weights are calculated following the [Nguyen-Widrow initialization algorithm](https://web.stanford.edu/class/ee373b/nninitialization.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [0. 1.]]\n",
      "1\n",
      "Trainer(TrainBFGS)\n",
      "<neurolab.error.SSE object at 0x000002017F7252B0>\n",
      "2\n",
      "2\n",
      "2\n",
      "{'w': array([[-6.18534131,  4.9458622 ],\n",
      "       [ 1.73966637, -7.72616081]]), 'b': array([5.19927709, 9.94629242])}\n",
      "2\n",
      "1\n",
      "1\n",
      "{'w': array([[-2.93909004, -4.76673365]]), 'b': array([7.70582369])}\n"
     ]
    }
   ],
   "source": [
    "print(my_net.inp_minmax)\n",
    "print(my_net.co)\n",
    "print(my_net.trainf)\n",
    "print(my_net.errorf)\n",
    "hidden_layer = my_net.layers[0]\n",
    "print(hidden_layer.ci)\n",
    "print(hidden_layer.cn)\n",
    "print(hidden_layer.co)\n",
    "print(hidden_layer.np)\n",
    "output_layer = my_net.layers[1]\n",
    "print(output_layer.ci)\n",
    "print(output_layer.cn)\n",
    "print(output_layer.co)\n",
    "print(output_layer.np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to modify the initialization of the biases and weights, you may find available initialization options at https://pythonhosted.org/neurolab/lib.html#module-neurolab.init.<br>\n",
    "Let us for example set all of them to zero, using the following instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w': array([[0., 0.],\n",
      "       [0., 0.]]), 'b': array([0., 0.])}\n",
      "{'w': array([[0., 0.]]), 'b': array([0.])}\n"
     ]
    }
   ],
   "source": [
    "from neurolab import init\n",
    "\n",
    "for l in my_net.layers:\n",
    "    l.initf = init.init_zeros\n",
    "my_net.init()\n",
    "print(hidden_layer.np)\n",
    "print(output_layer.np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to modify the training algorithm, you may find available implemented options at https://pythonhosted.org/neurolab/lib.html#module-neurolab.train.<br>\n",
    "Let us for example switch to the _gradient descent backpropagation_, using the following instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurolab import train\n",
    "\n",
    "my_net.trainf = train.train_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can also modify the error function to be used when training, you may find available options at https://pythonhosted.org/neurolab/lib.html#module-neurolab.error.<br>\n",
    "Let us for example choose the _mean squared error_, using the following instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurolab import error\n",
    "\n",
    "my_net.errorf = error.MSE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us train our neural network so that it models the behaviour of the _xor_ logic gate.\n",
    "\n",
    "First, we need to split our training set into two components: on one hand an array or a list of lists with the input data corresponding to each example, *xor_in* , and on the other hand an array or list of lists with the correct expected ouput for each example, *xor_out* (remember that this time the training set is **not** linearly separable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_in = numpy.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "xor_out = numpy.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us measure which is the error associated to the initial neural network before the training starts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "print(my_net.sim(xor_in))\n",
    "print(my_net.errorf(xor_out, my_net.sim(xor_in)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now proceed to run the training process on the neural network. The functions involved in the training work over the following arguments:\n",
    "* `lr`: _learning rate_, default value 0.01.\n",
    "* `epochs`: maximum number of epochs, default value 500.\n",
    "* `show`: number of epochs that should be executed between two messages in the output log, default value 100.\n",
    "* `goal`: maximum error accepted (halting criterion), default value 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10; Error: 0.25;\n",
      "Epoch: 20; Error: 0.25;\n",
      "Epoch: 30; Error: 0.25;\n",
      "Epoch: 40; Error: 0.25;\n",
      "Epoch: 50; Error: 0.25;\n",
      "The maximum number of train epochs is reached\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5],\n",
       "       [0.5],\n",
       "       [0.5],\n",
       "       [0.5]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_net.train(xor_in, xor_out, lr=0.1, epochs=50, show=10, goal=0.001)\n",
    "my_net.sim(xor_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now try a different setting. If we reset the neural network and we choose random numbers as initial values for the weights, we obtain the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000; Error: 0.20624769635154383;\n",
      "Epoch: 2000; Error: 0.11489105507175529;\n",
      "Epoch: 3000; Error: 0.01232716148235405;\n",
      "Epoch: 4000; Error: 0.005060602419320548;\n",
      "Epoch: 5000; Error: 0.003064045758926594;\n",
      "Epoch: 6000; Error: 0.0021673030896454206;\n",
      "Epoch: 7000; Error: 0.00166543486478401;\n",
      "Epoch: 8000; Error: 0.0013471000008847842;\n",
      "Epoch: 9000; Error: 0.0011281639233705698;\n",
      "The goal of learning is reached\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.02815856],\n",
       "       [0.96705024],\n",
       "       [0.96701362],\n",
       "       [0.03213734]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.random.seed(3287426346)  # we set this init seed only for class, so that we always get\n",
    "                               # the same random numbers and we can compare\n",
    "my_net.reset()\n",
    "for l in my_net.layers:\n",
    "    l.initf = init.InitRand([-1, 1], 'bw')  # 'b' means biases will be modified,\n",
    "                                            # and 'w' the weights\n",
    "my_net.init()\n",
    "my_net.train(xor_in, xor_out, lr=0.1, epochs=10000, show=1000, goal=0.001)\n",
    "my_net.sim(xor_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Iris_ dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Iris_ is a classic multivariant dataset that has been exhaustively studied and has become a standard reference when analysing the behaviour of different machine learning algorithms.\n",
    "\n",
    "_Iris_ gathers four measurements (length and width of sepal and petal) of 50 flowers of each one of the following three species of lilies: _Iris setosa_, _Iris virginica_ and _Iris versicolor_.\n",
    "\n",
    "Let us start by reading the data from the file `iris.csv` that has been provided together with the practice. It suffices to evaluate the following expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal width</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal length  sepal width  petal length  petal width      Species\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa\n",
       "5           5.4          3.9           1.7          0.4  Iris-setosa\n",
       "6           4.6          3.4           1.4          0.3  Iris-setosa\n",
       "7           5.0          3.4           1.5          0.2  Iris-setosa\n",
       "8           4.4          2.9           1.4          0.2  Iris-setosa\n",
       "9           4.9          3.1           1.5          0.1  Iris-setosa"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "iris = pandas.read_csv('iris.csv', header=None,\n",
    "                       names=['Sepal length', 'sepal width',\n",
    "                              'petal length', 'petal width',\n",
    "                              'Species'])\n",
    "iris.head(10)  # Display ten first examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us move to use a numerical version of the species instead.<br>\n",
    "Then, we should distribute the examples into two groups: training and test, and split each group into two components: input and expected output (goal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this piece of code might cause an error if wrong version of sklearn\n",
    "#from sklearn import preprocessing\n",
    "#from sklearn import model_selection\n",
    "\n",
    "#iris_training, iris_test = model_selection.train_test_split(\n",
    "#    iris, test_size=.33, random_state=2346523,\n",
    "#    stratify=iris['Species'])\n",
    "\n",
    "#ohe = preprocessing.OneHotEncoder(sparse = False)\n",
    "#input_training = iris_training.iloc[:, :4]\n",
    "#goal_training = ohe.fit_transform(iris_training['Species'].values.reshape(-1, 1))\n",
    "#input_training = iris_test.iloc[:, :4]\n",
    "#goal_training = ohe.transform(iris_test['Species'].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "#################\n",
    "#try this instead if the previous does not work\n",
    "import pandas\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "\n",
    "iris2 = pandas.read_csv('iris_enc.csv', header=None,\n",
    "                       names=['Sepal length', 'sepal width',\n",
    "                              'petal length', 'petal width',\n",
    "                              'Species'])\n",
    "#iris2.head(10)  # Display ten first examples\n",
    "iris_training, iris_test = model_selection.train_test_split(\n",
    "    iris2, test_size=.33, random_state=2346523,\n",
    "    stratify=iris['Species'])\n",
    "\n",
    "ohe = preprocessing.OneHotEncoder(sparse = False)\n",
    "\n",
    "input_training = iris_training.iloc[:, :4]\n",
    "goal_training = ohe.fit_transform(iris_training['Species'].values.reshape(-1, 1))\n",
    "goal_training[:10]  # this displays the 10 first expected output vectors (goal)\n",
    "                    # associated with the training set examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test = iris_test.iloc[:, :4]\n",
    "goal_test = ohe.transform(iris_test['Species'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Sepal length  sepal width  petal length  petal width\n",
      "45            4.8          3.0           1.4          0.3\n",
      "50            7.0          3.2           4.7          1.4\n",
      "136           6.3          3.4           5.6          2.4\n",
      "124           6.7          3.3           5.7          2.1\n",
      "8             4.4          2.9           1.4          0.2\n",
      "121           5.6          2.8           4.9          2.0\n",
      "133           6.3          2.8           5.1          1.5\n",
      "49            5.0          3.3           1.4          0.2\n",
      "131           7.9          3.8           6.4          2.0\n",
      "11            4.8          3.4           1.6          0.2\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(input_training.head(10))\n",
    "print(goal_training[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Sepal length  sepal width  petal length  petal width\n",
      "57            4.9          2.4           3.3          1.0\n",
      "32            5.2          4.1           1.5          0.1\n",
      "87            6.3          2.3           4.4          1.3\n",
      "17            5.1          3.5           1.4          0.3\n",
      "79            5.7          2.6           3.5          1.0\n",
      "138           6.0          3.0           4.8          1.8\n",
      "73            6.1          2.8           4.7          1.2\n",
      "82            5.8          2.7           3.9          1.2\n",
      "33            5.5          4.2           1.4          0.2\n",
      "2             4.7          3.2           1.3          0.2\n",
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(input_test.head(10))\n",
    "print(goal_test[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1__: define a function **lily_species** that, given an array with three numbers as input, returns the position where the maximum value is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def lily_species1(a):\n",
    "    valM = max(a)\n",
    "    for i in range(len(a)):\n",
    "        if(a[i] == valM):\n",
    "            return i\n",
    "        \n",
    "def lily_species2(a):\n",
    "    b=list(a)\n",
    "    return b.index(max(b))\n",
    "\n",
    "def lily_species3(a):\n",
    "    return numpy.argmax(a)\n",
    "\n",
    "print(lily_species1(numpy.array([2, 5, 0])))\n",
    "print(lily_species2(numpy.array([2, 5, 0])))\n",
    "print(lily_species3(numpy.array([2, 5, 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 2__: Create a feed forward neural network having the following features:\n",
    "1. Has four input neurons, one for each attribute of the iris dataset.\n",
    "2. Has three output neurons, one for each species.\n",
    "3. Has one hidden layer with two neurons.\n",
    "4. All neurons of all layers use the sigmoid as activation function.\n",
    "5. The initial biases and weights are all equal to zero.\n",
    "6. Training method is gradient descent backpropagation.\n",
    "7. The error function is the mean squared error.\n",
    "\n",
    "Once you have created it, train the network over the sets `input_training` and `goal_training`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurolab import net, trans, init, train, error\n",
    "\n",
    "sigmoid_act_fun = trans.LogSig()\n",
    "netEx2 = net.newff(minmax=[[4.0, 8.5], [1.5, 5.0], [0.5, 7.5], [0.0, 3.0]], size=[2,3], \n",
    "                   transf=[sigmoid_act_fun]*2)\n",
    "for l in netEx2.layers:\n",
    "    l.initf = init.init_zeros\n",
    "    \n",
    "netEx2.init()\n",
    "\n",
    "netEx2.trainf = train.train_gd\n",
    "\n",
    "netEx2.errorf = error.MSE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3__: Calculate the performance of the network that was trained on the previous exercise, using to this aim the sets `input_test` and `goal_test`. That is, calculate which fraction of the test set is getting the correct classification predicted by the network.<br>\n",
    "__Hint:__ In order to translate the output of the network and obtain which is the species predicted, use the function from exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of train epochs is reached\n",
      "Result: 68.0%\n"
     ]
    }
   ],
   "source": [
    "netEx2.init()\n",
    "netEx2.train(input_training, goal_training, lr=0.1, epochs=50, show=100, goal=0.001)\n",
    "flist = netEx2.sim(input_test)\n",
    "\n",
    "res = 0\n",
    "for i in range(len(flist)):\n",
    "    if str(lily_species1(flist[i].tolist())) == str(lily_species1(goal_test[i].tolist())):\n",
    "        res += 1\n",
    "\n",
    "print(\"Result: \" + str(res / len(flist) * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 4__: try to create different variants of the network from exercise 2, by modifying the number of hidden layers and/or the amount of neurons per layer, in such a way that the performance over the test set is improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of train epochs is reached\n",
      "Result: 78.0%\n"
     ]
    }
   ],
   "source": [
    "from neurolab import net, trans, init, train, error\n",
    "\n",
    "sigmoid_act_fun = trans.LogSig()\n",
    "netEx4 = net.newff(minmax=[[4.0, 4.5], [1.5, 5.0], [3.5, 7.5], [0.0, 3.0]], size=[3,3], \n",
    "                   transf=[sigmoid_act_fun]*2)\n",
    "for l in netEx4.layers:\n",
    "    l.initf = init.init_zeros\n",
    "    \n",
    "netEx4.init()\n",
    "\n",
    "netEx4.trainf = train.train_gd\n",
    "\n",
    "netEx4.errorf = error.MSE()\n",
    "\n",
    "netEx4.init()\n",
    "netEx4.train(input_training, goal_training, lr=0.1, epochs=50, show=100, goal=0.001)\n",
    "flist = netEx4.sim(input_test)\n",
    "\n",
    "res = 0\n",
    "for i in range(len(flist)):\n",
    "    if str(lily_species1(flist[i].tolist())) == str(lily_species1(goal_test[i].tolist())):\n",
    "        res += 1\n",
    "\n",
    "print(\"Result: \" + str(res / len(flist) * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
